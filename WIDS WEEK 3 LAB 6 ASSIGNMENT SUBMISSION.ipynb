{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfsRIWtJiCCVeOYwtZuZtl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K-2315/WIDS-/blob/main/WIDS%20WEEK%203%20LAB%206%20ASSIGNMENT%20SUBMISSION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bvz5pCN0q-0s",
        "outputId": "82452064-4e7f-4c1e-d4af-7c8418c73596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving basic OLSLR problem:\n",
            "Converged in 2 iterations.\n",
            "Minimizer values =  [[29.95996856]\n",
            " [34.88593338]\n",
            " [ 7.23429824]\n",
            " [94.76512444]\n",
            " [11.09213557]\n",
            " [95.50624453]\n",
            " [40.70708442]\n",
            " [80.82025945]\n",
            " [52.26164555]\n",
            " [ 6.60582702]]\n",
            "Value of Square of Euclidean Norm of (Ax* - y) for basic OLSLR  1.882944611344992\n",
            "\n",
            "Solving regularized OLSLR problem:\n",
            "Converged in 2 iterations.\n",
            "Minimizer values =  [[29.9598925 ]\n",
            " [34.88573686]\n",
            " [ 7.23427589]\n",
            " [94.7646815 ]\n",
            " [11.09202598]\n",
            " [95.50569292]\n",
            " [40.70678875]\n",
            " [80.81984761]\n",
            " [52.26141276]\n",
            " [ 6.6057882 ]]\n",
            "Value of Square of Euclidean Norm of (Ax* - y) for regularised OLSLR  1.8831075222043496\n",
            "Time taken =  0.019102087000000267\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "import time\n",
        "\n",
        "np.random.seed(42)\n",
        "A, y = make_regression(n_samples=200, n_features=10, noise=0.1)\n",
        "\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "def newton_ols(A, y, x0, tol=1e-6, max_iter=100):\n",
        "    x = x0\n",
        "    for i in range(max_iter):\n",
        "        grad = A.T @ (A @ x - y)\n",
        "        hessian = A.T @ A\n",
        "        if np.linalg.det(hessian) == 0:\n",
        "            print(\"Hessian is not invertible at iteration\", i)\n",
        "            break\n",
        "        delta_x = np.linalg.inv(hessian) @ grad\n",
        "        x = x - delta_x\n",
        "        if np.linalg.norm(delta_x) < tol:\n",
        "            print(f\"Converged in {i + 1} iterations.\")\n",
        "            return x\n",
        "\n",
        "    print(\"Did not converge within the maximum number of iterations.\")\n",
        "    return x\n",
        "\n",
        "\n",
        "def newton_regularized_ols(A, y, x0, lam, tol=1e-6, max_iter=100):\n",
        "    x = x0\n",
        "    for i in range(max_iter):\n",
        "        grad = A.T @ (A @ x - y) + lam * x\n",
        "        hessian = A.T @ A + lam * np.eye(A.shape[1])\n",
        "        if np.linalg.det(hessian) == 0:\n",
        "            print(\"Hessian is not invertible at iteration\", i)\n",
        "            break\n",
        "        delta_x = np.linalg.inv(hessian) @ grad\n",
        "        x = x - delta_x\n",
        "        if np.linalg.norm(delta_x) < tol:\n",
        "            print(f\"Converged in {i + 1} iterations.\")\n",
        "            return x\n",
        "\n",
        "    print(\"Did not converge within the maximum number of iterations.\")\n",
        "    return x\n",
        "t0=time.process_time()\n",
        "x0 = np.zeros((A.shape[1], 1))\n",
        "\n",
        "print(\"Solving basic OLSLR problem:\")\n",
        "x_ols = newton_ols(A, y, x0)\n",
        "print(\"Minimizer values = \",x_ols)\n",
        "print(\"Value of Square of Euclidean Norm of (Ax* - y) for basic OLSLR \",np.linalg.norm(A @ x_ols - y) ** 2)\n",
        "\n",
        "print(\"\\nSolving regularized OLSLR problem:\")\n",
        "lambda_reg = 0.001\n",
        "x_reg_ols = newton_regularized_ols(A, y, x0, lambda_reg)\n",
        "print(\"Minimizer values = \",x_reg_ols)\n",
        "print(\"Value of Square of Euclidean Norm of (Ax* - y) for regularised OLSLR \",np.linalg.norm(A @ x_reg_ols - y) ** 2)\n",
        "t1=time.process_time()\n",
        "print(\"Time taken = \",t1-t0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "A, y = make_regression(n_samples=200, n_features=10, noise=0.1)\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "def bfgs_ols(A, y, x0):\n",
        "    def objective(x):\n",
        "        x = x.reshape(-1, 1)\n",
        "        return 0.5 * np.linalg.norm(A @ x - y) ** 2\n",
        "\n",
        "    def gradient(x):\n",
        "        x = x.reshape(-1, 1)\n",
        "        return (A.T @ (A @ x - y)).flatten()\n",
        "\n",
        "    result = minimize(objective, x0.flatten(), jac=gradient, method='BFGS')\n",
        "    if result.success:\n",
        "        print(\"Converged in\", result.nit, \"iterations.\")\n",
        "    else:\n",
        "        print(\"Optimization did not converge.\")\n",
        "    return result.x.reshape(-1, 1)\n",
        "\n",
        "def bfgs_regularized_ols(A, y, x0, lam):\n",
        "    def objective(x):\n",
        "        x = x.reshape(-1, 1)\n",
        "        return 0.5 * np.linalg.norm(A @ x - y) ** 2 + 0.5 * lam * np.linalg.norm(x) ** 2\n",
        "\n",
        "    def gradient(x):\n",
        "        x = x.reshape(-1, 1)\n",
        "        return (A.T @ (A @ x - y) + lam * x).flatten()\n",
        "\n",
        "    result = minimize(objective, x0.flatten(), jac=gradient, method='BFGS')\n",
        "    if result.success:\n",
        "        print(\"Converged in\", result.nit, \"iterations.\")\n",
        "    else:\n",
        "        print(\"Optimization did not converge.\")\n",
        "    return result.x.reshape(-1, 1)\n",
        "\n",
        "t0=time.process_time()\n",
        "x0 = np.zeros((A.shape[1], 1))\n",
        "\n",
        "print(\"Solving basic OLSLR problem with BFGS:\")\n",
        "x_ols_bfgs = bfgs_ols(A, y, x0)\n",
        "print(\"Minimizer values = \",x_ols_bfgs)\n",
        "print(\"Value of Square of Euclidean Norm of (Ax* - y) for basic OLSLR \",np.linalg.norm(A @ x_ols_bfgs - y) ** 2)\n",
        "print(\"\\nSolving regularized OLSLR problem with BFGS:\")\n",
        "lambda_reg = 0.001\n",
        "x_reg_ols_bfgs = bfgs_regularized_ols(A, y, x0, lambda_reg)\n",
        "print(\"Minimizer values = \",x_reg_ols_bfgs)\n",
        "print(\"Value of Square of Euclidean Norm of (Ax* - y) for regularised OLSLR \",np.linalg.norm(A @ x_reg_ols_bfgs - y) ** 2)\n",
        "t1=time.process_time()\n",
        "print(\"Time taken = \",t1-t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-qVdv8-pEoX",
        "outputId": "774de1ae-f17d-4435-dec1-95c61beb0110"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving basic OLSLR problem with BFGS:\n",
            "Converged in 16 iterations.\n",
            "Minimizer values =  [[29.95996856]\n",
            " [34.88593338]\n",
            " [ 7.23429824]\n",
            " [94.76512444]\n",
            " [11.09213557]\n",
            " [95.50624452]\n",
            " [40.70708442]\n",
            " [80.82025945]\n",
            " [52.26164555]\n",
            " [ 6.60582702]]\n",
            "Value of Square of Euclidean Norm of (Ax* - y) for basic OLSLR  1.8829446113450894\n",
            "\n",
            "Solving regularized OLSLR problem with BFGS:\n",
            "Converged in 16 iterations.\n",
            "Minimizer values =  [[29.9598925 ]\n",
            " [34.88573686]\n",
            " [ 7.23427589]\n",
            " [94.7646815 ]\n",
            " [11.09202598]\n",
            " [95.50569292]\n",
            " [40.70678875]\n",
            " [80.81984762]\n",
            " [52.26141276]\n",
            " [ 6.6057882 ]]\n",
            "Value of Square of Euclidean Norm of (Ax* - y) for regularised OLSLR  1.883107522182522\n",
            "Time taken =  0.02233640500000078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import T\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "A, y = np.random.randn(10, 10), np.random.randn(10, 1)\n",
        "lambda_reg = 0.001\n",
        "N, d = A.shape\n",
        "\n",
        "\n",
        "def compute_fi(x, i):\n",
        "    a_i = A[i]\n",
        "    y_i = y[i]\n",
        "    term2 = 0.5 * (a_i @ x - y_i) ** 2\n",
        "    term1 = (lambda_reg / (2 * N)) * np.linalg.norm(x) **2\n",
        "    return term1 + term2\n",
        "\n",
        "\n",
        "def compute_grad_fi(x, i):\n",
        "    a_i = A[i].reshape(1, -1)\n",
        "    y_i = y[i]\n",
        "    grad_term2 = (a_i @ x - y_i) * a_i.T\n",
        "    grad_term1 = (lambda_reg / N) * x\n",
        "    return grad_term1 + grad_term2\n",
        "\n",
        "\n",
        "x = np.random.randn(d, 1)\n",
        "for i in range(N):\n",
        "  fi_value = compute_fi(x, i)\n",
        "  grad_fi_value = compute_grad_fi(x, i)\n",
        "  print(f\"f_{i}(x): {fi_value}\")\n",
        "  print(f\"Gradient of f_{i}(x):\\n{grad_fi_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdR-v8uQJhOJ",
        "outputId": "75a5d35a-9116-48d9-de56-baf2f5c9a9dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f_0(x): [4.96422949]\n",
            "Gradient of f_0(x):\n",
            "[[ 1.56482405]\n",
            " [-0.43563717]\n",
            " [ 2.04070261]\n",
            " [ 4.79891365]\n",
            " [-0.73777506]\n",
            " [-0.73767394]\n",
            " [ 4.97568161]\n",
            " [ 2.41786854]\n",
            " [-1.47907628]\n",
            " [ 1.70953948]]\n",
            "f_1(x): [15.93721128]\n",
            "Gradient of f_1(x):\n",
            "[[ 2.61609182]\n",
            " [ 2.62933408]\n",
            " [-1.36602285]\n",
            " [10.80191401]\n",
            " [ 9.73822442]\n",
            " [ 3.17449613]\n",
            " [ 5.71806382]\n",
            " [-1.77424027]\n",
            " [ 5.12648003]\n",
            " [ 7.9734159 ]]\n",
            "f_2(x): [25.45965483]\n",
            "Gradient of f_2(x):\n",
            "[[-10.45859953]\n",
            " [  1.61106587]\n",
            " [ -0.48185396]\n",
            " [ 10.16680031]\n",
            " [  3.8845243 ]\n",
            " [ -0.79147848]\n",
            " [  8.21312391]\n",
            " [ -2.68097951]\n",
            " [  4.28608266]\n",
            " [  2.08150985]]\n",
            "f_3(x): [0.0492521]\n",
            "Gradient of f_3(x):\n",
            "[[ 0.18739416]\n",
            " [-0.5774627 ]\n",
            " [ 0.00421387]\n",
            " [ 0.32999474]\n",
            " [-0.25645309]\n",
            " [ 0.38063629]\n",
            " [-0.06511809]\n",
            " [ 0.61082333]\n",
            " [ 0.41418512]\n",
            " [-0.06129761]]\n",
            "f_4(x): [8.38090864]\n",
            "Gradient of f_4(x):\n",
            "[[-3.02344423]\n",
            " [-0.70157747]\n",
            " [ 0.47346542]\n",
            " [ 1.23295269]\n",
            " [ 6.05299007]\n",
            " [ 2.94704327]\n",
            " [ 1.88583305]\n",
            " [-4.32793256]\n",
            " [-1.40664512]\n",
            " [ 7.21789038]]\n",
            "f_5(x): [2.07202813]\n",
            "Gradient of f_5(x):\n",
            "[[ 0.65943996]\n",
            " [-0.78378882]\n",
            " [-1.37778291]\n",
            " [ 1.24523588]\n",
            " [ 2.09844961]\n",
            " [ 1.8955328 ]\n",
            " [-1.70812437]\n",
            " [-0.62947947]\n",
            " [ 0.67435901]\n",
            " [ 1.98567365]]\n",
            "f_6(x): [12.3235619]\n",
            "Gradient of f_6(x):\n",
            "[[ 2.37864698]\n",
            " [ 0.92169295]\n",
            " [ 5.49235603]\n",
            " [ 5.93876008]\n",
            " [-4.03376698]\n",
            " [-6.73296221]\n",
            " [ 0.35748753]\n",
            " [-4.98211061]\n",
            " [-1.79521143]\n",
            " [ 3.20274308]]\n",
            "f_7(x): [3.5163973]\n",
            "Gradient of f_7(x):\n",
            "[[ 0.95811879]\n",
            " [ 4.07840013]\n",
            " [-0.09499368]\n",
            " [ 4.14920297]\n",
            " [-6.94678237]\n",
            " [ 2.17946433]\n",
            " [ 0.23081874]\n",
            " [-0.79299295]\n",
            " [ 0.24343581]\n",
            " [-5.27034995]]\n",
            "f_8(x): [1.38571896]\n",
            "Gradient of f_8(x):\n",
            "[[ 0.36542332]\n",
            " [-0.59436993]\n",
            " [-2.45975579]\n",
            " [ 0.86283948]\n",
            " [ 1.34561288]\n",
            " [ 0.83513929]\n",
            " [-1.52357086]\n",
            " [-0.5472802 ]\n",
            " [ 0.88183102]\n",
            " [-0.85419148]]\n",
            "f_9(x): [1.69108954]\n",
            "Gradient of f_9(x):\n",
            "[[-0.17868967]\n",
            " [-1.7810632 ]\n",
            " [ 1.29088048]\n",
            " [ 0.60272311]\n",
            " [ 0.72095529]\n",
            " [ 2.69101474]\n",
            " [-0.5444838 ]\n",
            " [-0.4801227 ]\n",
            " [-0.0092879 ]\n",
            " [ 0.43141371]]\n"
          ]
        }
      ]
    }
  ]
}